#  System Architecture Documentation

## Overview

The AI Code Reviewer uses a **state machine architecture** powered by LangGraph to orchestrate code review workflows with DeepSeek-R1 running locally via Ollama.

## Architecture Diagram

```
┌─────────────────────────────────────────────────────────────┐
│                     User Interface Layer                     │
│                     (examples/review.py)                     │
└────────────────────────────┬────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────┐
│                    LangGraph Workflow                        │
│                    (src/workflow.py)                         │
│                                                              │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐  │
│  │   Parser     │───▶│ Bug Detector │───▶│  Optimizer   │  │
│  │   Node       │    │     Node     │    │     Node     │  │
│  └──────────────┘    └──────┬───────┘    └──────┬───────┘  │
│                             │                    │          │
│                             ▼                    ▼          │
│                    ┌──────────────────────────────┐         │
│                    │  Report Generator Node       │         │
│                    └──────────────────────────────┘         │
└─────────────────────────────┬────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                    LLM Integration Layer                     │
│                  (src/deepseek_agent.py)                     │
│                                                              │
│         ┌──────────────────────────────────────┐            │
│         │      Ollama API Client               │            │
│         │  - Query Management                  │            │
│         │  - JSON Parsing                      │            │
│         │  - Error Handling                    │            │
│         └──────────────┬───────────────────────┘            │
└────────────────────────┼────────────────────────────────────┘
                         │
                         ▼
              ┌──────────────────────┐
              │   Ollama Server      │
              │   (localhost:11434)  │
              │                      │
              │   DeepSeek-R1 Model  │
              │   Running on GPU     │
              └──────────────────────┘
```

## Component Details

### 1. State Management (`src/state.py`)

**Purpose**: Define the data structure that flows through the workflow.

```python
ReviewState = {
    "file_path": str,           # Input file location
    "code_content": str,        # Raw code content
    "language": str,            # Detected language
    "bugs": List[Dict],         # Bug findings
    "optimizations": List[Dict], # Optimization suggestions
    "report": Dict,             # Final report
    "metadata": Dict            # Additional info
}
```

### 2. Workflow Orchestration (`src/workflow.py`)

**Purpose**: Define the sequential execution flow using LangGraph.

- **Entry Point**: `parse_code`
- **Flow**: Parse → Detect → Optimize → Report
- **Exit Point**: Report generation complete

**Key Function**: `create_review_workflow()`
- Creates StateGraph instance
- Registers all nodes
- Defines edges (execution order)
- Returns compiled graph

### 3. Node Implementations (`src/nodes.py`)

Each node is a pure function that takes `ReviewState` and returns modified `ReviewState`.

#### Node 1: Parser
- Reads file from disk
- Detects programming language
- Validates file size
- Populates metadata

#### Node 2: Bug Detector
- Formats code with bug detection prompt
- Sends to DeepSeek-R1 via Ollama
- Parses JSON response
- Filters by confidence threshold (≥0.7)

#### Node 3: Optimizer
- Formats code with optimization prompt
- Sends to DeepSeek-R1 via Ollama
- Extracts optimization suggestions
- Categories: performance, security, best practices

#### Node 4: Report Generator
- Compiles all findings
- Calculates severity distribution
- Determines overall severity level
- Formats final JSON report

### 4. LLM Integration (`src/deepseek_agent.py`)

**Purpose**: Abstract Ollama API interactions.

**Key Methods**:
- `query()`: Send prompt and parse response
- `detect_bugs()`: Specialized bug detection
- `suggest_optimizations()`: Specialized optimization analysis

**Features**:
- JSON extraction from markdown code blocks
- Error handling and retry logic
- Configuration management (temperature, tokens)

### 5. Prompt Engineering (`src/prompts.py`)

**Purpose**: Define specialized prompts for each task.

**BUG_DETECTION_PROMPT**:
- Focuses on logic errors, vulnerabilities, memory issues
- Requests structured JSON output
- Includes severity and confidence scoring

**OPTIMIZATION_PROMPT**:
- Focuses on performance, code quality, best practices
- Requests categorized suggestions
- Includes impact assessment

### 6. Report Generation (`src/report_generator.py`)

**Purpose**: Format and save review results.

**Outputs**:
- **JSON**: Machine-readable format for CI/CD integration
- **Markdown**: Human-readable format for documentation
- **Console**: Real-time summary display

## Data Flow

```
1. User Input
    File path provided via CLI

2. Parse Node
    Read file → Detect language → Create metadata

3. Bug Detection Node
    Format prompt → Call LLM → Parse JSON → Filter results

4. Optimization Node
    Format prompt → Call LLM → Parse JSON → Categorize

5. Report Generation Node
    Aggregate findings → Calculate metrics → Format output

6. Save Reports
    Write JSON file → Write Markdown file → Display summary
```

## Configuration (`config.yaml`)

**Model Settings**:
- Model name (deepseek-r1:latest)
- Temperature (0.1 for consistent results)
- Max tokens (2048)

**Review Settings**:
- Supported languages
- Confidence thresholds
- Check categories

**Output Settings**:
- Format preferences
- Report options

## Performance Characteristics

- **Latency**: ~5-10 seconds per file (depends on file size and GPU)
- **Memory**: ~8GB VRAM for DeepSeek-R1
- **Accuracy**: 85% on test dataset
- **Throughput**: Sequential processing (1 file at a time)

## Extensibility Points

1. **Add New Languages**: Update `ext_to_lang` in `nodes.py`
2. **Custom Prompts**: Modify `prompts.py`
3. **New Node Types**: Add nodes in `nodes.py` and update workflow
4. **Different LLMs**: Replace `deepseek_agent.py` with new agent
5. **Output Formats**: Add formatters in `report_generator.py`

## Error Handling

- **File Not Found**: Graceful error with clear message
- **LLM Timeout**: Configurable timeout with retry
- **JSON Parse Error**: Fallback to raw response logging
- **VRAM Overflow**: Recommendation to use smaller model

## Security Considerations

- **Local Execution**: No data sent to external APIs
- **File Validation**: Size limits and type checking
- **Injection Prevention**: Sanitized inputs to LLM
- **Output Sandboxing**: Reports saved to dedicated directory

